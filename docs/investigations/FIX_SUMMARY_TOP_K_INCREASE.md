# Fix Summary: Revenue Query Retrieval Issue

**Date**: 2025-10-21
**Issue**: RAG system failed to answer "What was IBM revenue in 2021?"
**Root Cause**: Revenue chunk ranked #19, outside default top_k=5
**Fix Applied**: Increased default top_k from 5 to 20

## Test Results

### Before Fix (top_k=5)
- ‚ùå Revenue chunk NOT retrieved (rank #19)
- ‚ùå Answer: "The information is not available in the provided documents"
- ‚ùå User gets incorrect response

### After Fix (top_k=20)
- ‚úÖ Revenue chunk retrieved at rank #19
- ‚úÖ Answer: "IBM generated $57.4 billion in revenue in 2021"
- ‚úÖ User gets correct answer
- ‚ö†Ô∏è LLM still generates additional questions (Issue #461 - separate fix in progress)

## Changes Made

**File**: `backend/core/config.py:45`

```python
# Before:
number_of_results: Annotated[int, Field(default=5, alias="NUMBER_OF_RESULTS")]

# After:
# NOTE: Temporarily set to 20 due to embedding model ranking issues (revenue chunk ranks #19)
# TODO: Implement hybrid search (vector + BM25) to reduce this to 10
# TODO: Evaluate better embedding models (bge-large-en, e5-large-v2)
number_of_results: Annotated[int, Field(default=20, alias="NUMBER_OF_RESULTS")]  # Increased from 5
```

## Is This Best Practice? NO.

Returning 20 chunks is **NOT ideal** - it's a workaround that exposes the real problem:

### The Real Issues:
1. **Poor Embedding Model** - `ibm/slate-125m-english-rtrvr` ranks irrelevant chunks higher than the revenue chunk
2. **No Hybrid Search** - Pure vector search misses exact keyword matches
3. **No Query-Type Routing** - Financial queries need different handling

### Why 20 Chunks is Problematic:
- **Higher Latency**: More chunks to process and send to LLM
- **More Noise**: LLM sees more irrelevant context
- **Masking Root Cause**: Hides the embedding model weakness
- **Resource Waste**: Unnecessary computation and token usage

## Recommended Next Steps

### Priority 1: Hybrid Search (1-2 days) ‚≠ê
**Impact**: Revenue chunk would rank top 3-5 instead of #19

Combine vector search with BM25 keyword matching:
- Query: "What was IBM revenue in 2021?"
- BM25 finds: "revenue" + "2021" ‚Üí ranks revenue chunk #1
- Vector finds: semantic similarity ‚Üí ranks related chunks
- Fusion: Combines both using Reciprocal Rank Fusion

**Result**: Could reduce top_k back to 10 while improving accuracy

### Priority 2: Better Embedding Model (1 week)
Evaluate and switch to:
- `BAAI/bge-large-en-v1.5` - SOTA general retrieval
- `intfloat/e5-large-v2` - Instruction-tuned for Q&A
- `sentence-transformers/all-mpnet-base-v2` - Strong baseline

**Test Framework**: `backend/test_embedding_limits.py`

### Priority 3: Query-Aware Retrieval (2 weeks)
Route different query types to specialized strategies:
- **Financial queries** ‚Üí Hybrid search with date/number boosting
- **General questions** ‚Üí Current vector search
- **Multi-hop reasoning** ‚Üí CoT with iterative retrieval

## Performance Considerations

### Current Setup (top_k=20):
- **Retrieval Time**: ~100-150ms (minimal impact)
- **LLM Processing**: More tokens, slightly higher latency
- **Cost**: ~2x token usage vs top_k=10

### Hybrid Search Approach (top_k=10):
- **Retrieval Time**: ~120-180ms (BM25 adds 20-30ms)
- **LLM Processing**: Fewer, more relevant chunks
- **Cost**: Similar to current, better quality

## Monitoring

Track these metrics going forward:
1. **Retrieval Recall@K**: % of queries where correct chunk is in top K
2. **Mean Reciprocal Rank (MRR)**: Average rank of correct chunk
3. **Query Latency**: End-to-end response time
4. **User Satisfaction**: Feedback on answer quality

## Rollback Plan

If this causes issues:
```python
# Revert to original value
number_of_results: Annotated[int, Field(default=5, alias="NUMBER_OF_RESULTS")]
```

Restart backend:
```bash
# Backend auto-reloads with --reload flag
touch backend/core/config.py
```

## Conclusion

‚úÖ **Immediate Problem Solved**: Query now works correctly

‚ö†Ô∏è **Workaround, Not Solution**: Top_k=20 is a band-aid

üéØ **Real Fix Needed**: Implement hybrid search + evaluate better embedding models

The current fix makes the system work but doesn't address the underlying ranking problem. Prioritize hybrid search implementation to reduce top_k while improving quality.
