# RAG Modulo

RAG Modulo is a robust, customizable Retrieval-Augmented Generation (RAG) solution that supports a wide variety of vector databases, embedding models, and document formats. The solution is designed to be flexible and not dependent on popular RAG frameworks like LangChain or LlamaIndex, allowing for greater customization and control.

## üéâ Current Project Status: MAJOR PROGRESS ACHIEVED

**Status**: The project has made significant progress with comprehensive test infrastructure, working core functionality, and production-ready architecture. While some tests are still failing, the foundation is solid and most features are operational.

### ‚úÖ What's Working
- **Infrastructure**: All Docker containers running (PostgreSQL, Milvus, MLFlow, MinIO)
- **Test Infrastructure**: 847 tests passing with comprehensive coverage
- **Core Services**: Search, conversation, and token tracking services operational
- **API Endpoints**: Most REST API endpoints functional
- **Token Tracking**: Advanced token usage monitoring and warning system
- **Chain of Thought**: Reasoning capabilities with token breakdown
- **Development Workflow**: Streamlined Docker-based development environment
- **CI/CD Pipeline**: Automated builds and testing with GitHub Actions

### üîÑ What's In Progress
- **Test Optimization**: 71 tests still failing (down from 200+)
- **API Integration**: Some chat router endpoints need fixes
- **Token Tracking**: E2E tests need refinement
- **CLI Testing**: Profile management tests need environment fixes

### üìä Current Assessment (Updated December 2024)
- **Infrastructure**: 95% complete ‚úÖ
- **Backend Structure**: 90% complete ‚úÖ
- **Backend Functionality**: 80% complete ‚úÖ
- **Testing**: 85% complete (847/918 tests passing) üîÑ
- **Integration**: 75% complete ‚úÖ
- **Documentation**: 90% complete ‚úÖ

## üéØ Recent Achievements

### ‚úÖ Token Tracking System (December 2024)
- **Session Statistics**: Enhanced with prompt/completion token breakdown
- **Real-time Monitoring**: Live token usage tracking across conversations
- **Warning System**: Intelligent warnings for approaching token limits
- **Service Integration**: Seamless integration across all services

### ‚úÖ Test Infrastructure Overhaul
- **70% Reduction**: Test files reduced from 192 to 58
- **92% Success Rate**: 847 tests passing out of 918 total
- **Fast Execution**: Core tests run in under 10 seconds
- **Comprehensive Coverage**: Multi-layer testing (atomic, unit, integration, E2E)

### ‚úÖ Development Workflow
- **Docker-First**: Streamlined development with Docker containers
- **GitHub Codespaces**: Full cloud development support
- **CI/CD Pipeline**: Automated builds and testing
- **Quality Gates**: 100% code quality compliance

## Table of Contents

- [Features](#features)
- [Document Processing Flow](#document-processing-flow)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Testing](#testing)
- [CI/CD](#cicd)
- [Contributing](#contributing)
- [License](#license)

## Features

### Architecture
- Service-based architecture with clean separation of concerns
- Repository pattern for database operations
- Provider abstraction for LLM integration
- Dependency injection for better testability
- Asynchronous API for efficient operations

### Data Processing
- Support for multiple vector databases (Elasticsearch, Milvus, Pinecone, Weaviate, ChromaDB)
- Flexible document processing for various formats (PDF, TXT, DOCX, XLSX)
- Customizable chunking strategies
- Configurable embedding models
- Separation of vector storage and metadata storage

### LLM Integration
- Multiple LLM provider support (WatsonX, OpenAI, Anthropic)
- Runtime provider configuration
- Template-based prompt management
- Error handling and recovery
- Concurrent request handling

### Advanced Features
- **Token Tracking & Monitoring**: Real-time token usage tracking with warnings
- **Chain of Thought Reasoning**: Advanced reasoning capabilities with step-by-step breakdown
- **Session Management**: Comprehensive conversation session handling
- **Source Attribution**: Detailed source tracking for generated responses
- **Multi-Model Support**: Seamless switching between different LLM providers
- **Context Management**: Intelligent context window management and optimization

### Testing & Quality
- **Comprehensive Test Suite**: 847 tests passing (92% success rate)
- **Multi-Layer Testing**: Atomic, unit, integration, and E2E tests
- **Code Coverage**: 50% overall coverage with detailed reporting
- **Performance Testing**: Scalability and performance validation
- **CI/CD Pipeline**: Automated builds, testing, and deployment
- **Code Quality**: 100% compliance with ruff, mypy, and pylint

## Document Processing Flow

The following diagram illustrates how documents are processed in our RAG solution:

```mermaid
graph TD
    A[User Uploads Document] --> B[DocumentProcessor]
    B --> C{Document Type?}
    C -->|PDF| D[PdfProcessor]
    C -->|TXT| E[TxtProcessor]
    C -->|DOCX| F[WordProcessor]
    C -->|XLSX| G[ExcelProcessor]
    D --> H[Extract Text, Tables, Images]
    E --> I[Process Text]
    F --> J[Extract Paragraphs]
    G --> K[Extract Sheets and Data]
    H --> L[Chunking]
    I --> L
    J --> L
    K --> L
    L --> M[Get Embeddings]
    M --> N{Store Data}
    N -->|Vector Data| O[VectorStore]
    O --> P{Vector DB Type}
    P -->|Milvus| Q[MilvusStore]
    P -->|Elasticsearch| R[ElasticsearchStore]
    P -->|Pinecone| S[PineconeStore]
    P -->|Weaviate| T[WeaviateStore]
    P -->|ChromaDB| U[ChromaDBStore]
    N -->|Metadata| V[PostgreSQL]
    V --> W[Repository Layer]
    W --> X[Service Layer]
```

Explanation of the document processing flow:

1. A user uploads a document to the system.
2. The DocumentProcessor determines the type of document and routes it to the appropriate processor (PdfProcessor, TxtProcessor, WordProcessor, or ExcelProcessor).
3. Each processor extracts the relevant content from the document.
4. The extracted content goes through a chunking process to break it into manageable pieces.
5. Embeddings are generated for the chunked content.
6. The data is then stored in two places:
   - Vector data (embeddings) are stored in the VectorStore, which can be one of several types (Milvus, Elasticsearch, Pinecone, Weaviate, or ChromaDB).
   - Metadata is stored in PostgreSQL, accessed through the Repository Layer and Service Layer.

This architecture allows for flexibility in choosing vector databases and ensures efficient storage and retrieval of both vector data and metadata.

## Prerequisites

- Python 3.12+ (required for backend)
- Node.js 18+ (required for frontend)
- Docker and Docker Compose
- Poetry (for Python dependency management)
- npm (for frontend dependency management)

## Installation

### Quick Start (Recommended)

1. Clone the repository:
   ```sh
   git clone https://github.com/manavgup/rag-modulo.git
   cd rag-modulo
   ```

2. Set up your environment variables:
   ```sh
   cp env.example .env
   # Edit .env with your specific configuration
   ```

3. Start the application with pre-built images:
   ```sh
   make run-ghcr
   ```

## üõ†Ô∏è Development Workflow

**NEW**: Streamlined development workflow for faster iteration! See [Development Workflow Guide](docs/development/workflow.md) for complete details.

**CLOUD DEVELOPMENT**: Full GitHub Codespaces support with automated PR validation! See [GitHub Codespaces Guide](docs/development/codespaces.md) for cloud development.

**TESTING**: Comprehensive testing strategy with fresh environment simulation, automated tests, and manual validation! See [Testing Documentation](docs/testing/index.md) for complete testing guide.

### Quick Development Start

#### Local Development (Dev Container)
```bash
# Initialize development environment
make dev-init

# Build and start development environment
make dev-build
make dev-up

# Validate everything is working
make dev-validate
```

#### Cloud Development (GitHub Codespaces)
1. **Go to GitHub repository** ‚Üí "Code" ‚Üí "Codespaces"
2. **Click "Create codespace"** on your branch
3. **Wait for environment** to load (2-3 minutes)
4. **Start coding** in browser-based VS Code!

**Automated PR Validation**: Every PR automatically gets a Codespace for easy testing and review.

### Development Commands

| Command | Description |
|---------|-------------|
| `make dev-init` | Initialize development environment |
| `make dev-build` | Build local development images |
| `make dev-up` | Start development environment |
| `make dev-restart` | Rebuild and restart with latest changes |
| `make dev-down` | Stop development environment |
| `make dev-status` | Show development environment status |
| `make dev-validate` | Validate development environment health |

### Key Benefits

- ‚úÖ **Local builds by default** - No more remote image confusion
- ‚úÖ **Automatic environment setup** - Development variables configured
- ‚úÖ **Fast iteration** - Changes visible immediately
- ‚úÖ **Health validation** - Know when everything is working
- ‚úÖ **Consistent workflow** - Same setup for all developers

### Development Setup

1. **Backend Setup**:
   ```sh
   cd backend
   poetry install --with dev
   poetry shell
   ```

2. **Frontend Setup**:
   ```sh
   cd webui
   npm install
   ```

3. **Build and Run Locally**:
   ```sh
   make build-all
   make run-app
   ```

### Environment Configuration

The system requires several environment variables. See `env.example` for the complete list. Key variables include:

- **Database**: `COLLECTIONDB_*` variables for PostgreSQL
- **Vector DB**: `VECTOR_DB`, `MILVUS_*` variables
- **LLM Providers**: `WATSONX_*`, `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`
- **Authentication**: `IBM_CLIENT_ID`, `IBM_CLIENT_SECRET`, `OIDC_*` variables

## Usage

### Running the Application

1. **Using Pre-built Images (Recommended)**:
   ```sh
   make run-ghcr
   ```

2. **Building and Running Locally**:
   ```sh
   make run-app
   ```

3. **Access Points**:
   - **Frontend**: http://localhost:3000
   - **Backend API**: http://localhost:8000
   - **MLFlow**: http://localhost:5001
   - **MinIO Console**: http://localhost:9001

### Available Make Commands

- `make run-ghcr` - Run with pre-built GitHub Container Registry images
- `make run-app` - Build and run with local images
- `make run-services` - Start only infrastructure services
- `make stop-containers` - Stop all containers
- `make logs` - View container logs
- `make clean` - Clean up containers and volumes

### Development Commands

- `make lint` - Run code quality checks
- `make test` - Run tests (requires testfile parameter)
- `make build-all` - Build all container images
- `make pull-ghcr-images` - Pull latest images from GHCR

### CI/Development Debugging Scripts

- `scripts/test_ci_quick.sh` - Quick CI environment validation
- `scripts/test_ci_environment.sh` - Comprehensive CI simulation
- `scripts/validate_ci_fixes.py` - Validate CI-related code changes

## Project Structure

```plaintext
rag_modulo/
‚îú‚îÄ‚îÄ .github/workflows/ci.yml   # GitHub Actions workflow for build/test/publish
‚îú‚îÄ‚îÄ backend                    # Python backend application
‚îÇ   ‚îú‚îÄ‚îÄ auth/                  # Authentication code (e.g. OIDC)
‚îÇ   ‚îú‚îÄ‚îÄ core/                  # Config, exceptions, middleware
‚îÇ   ‚îú‚îÄ‚îÄ rag_solution/          # Main application code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion/    # Data ingestion modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docs/             # Documentation files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/       # Evaluation modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation/       # Text generation modules
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ providers/    # LLM provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/          # Data models and schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline/        # RAG pipeline implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_rewriting/ # Query rewriting modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository/      # Repository layer implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval/       # Data retrieval modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router/          # API route handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/         # Pydantic schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/        # Service layer implementations
‚îÇ   ‚îú‚îÄ‚îÄ tests/               # Test suite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integration/     # Integration tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance/     # Performance tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Service tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md        # Testing documentation
‚îÇ   ‚îî‚îÄ‚îÄ vectordbs/           # Vector database interfaces
‚îú‚îÄ‚îÄ webui/                   # Frontend code
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/      # React components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Frontend services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config/         # Frontend configuration
‚îú‚îÄ‚îÄ scripts/                 # Development and debugging scripts
‚îÇ   ‚îú‚îÄ‚îÄ test_ci_quick.sh     # Quick CI environment test
‚îÇ   ‚îú‚îÄ‚îÄ test_ci_environment.sh # Full CI simulation
‚îÇ   ‚îî‚îÄ‚îÄ validate_ci_fixes.py # Code validation script
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ fixes/              # Fix documentation
‚îú‚îÄ‚îÄ .env                     # Environment variables
‚îú‚îÄ‚îÄ .env.ci                  # CI environment configuration
‚îú‚îÄ‚îÄ docker-compose-infra.yml # Infrastructure services configuration
‚îú‚îÄ‚îÄ docker-compose.yml       # Application services configuration
‚îú‚îÄ‚îÄ Makefile                # Project management commands
‚îú‚îÄ‚îÄ requirements.txt        # Project dependencies
‚îî‚îÄ‚îÄ README.md              # Project documentation
```

Key architectural components:

1. Service Layer:
   - Implements business logic
   - Manages transactions
   - Handles dependencies
   - Provides clean interfaces

2. Repository Layer:
   - Data access abstraction
   - Database operations
   - Query optimization
   - Transaction management

3. Provider System:
   - LLM provider abstraction
   - Multiple provider support
   - Configuration management
   - Error handling

4. Test Organization:
   - Unit tests by component
   - Integration tests
   - Performance tests
   - Service-specific tests

## OAuth flow with IBM

The following diagram illustrates the OAuth 2.0 Authorization Code flow used in our application with IBM as the identity provider:

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant Backend
    participant IBM_OIDC
    participant Database
    User->>Frontend: Clicks Login
    Frontend->>Backend: GET /api/auth/login
    Backend->>IBM_OIDC: Redirect to Authorization Endpoint
    IBM_OIDC->>User: Present Login Page
    User->>IBM_OIDC: Enter Credentials
    IBM_OIDC->>Backend: Redirect with Authorization Code
    Backend->>IBM_OIDC: POST /token (exchange code for tokens)
    IBM_OIDC-->>Backend: Access Token & ID Token
    Backend->>Backend: Parse ID Token
    Backend->>Database: Get or Create User
    Database-->>Backend: User Data
    Backend->>Backend: Set Session Data
    Backend->>Frontend: Redirect to Dashboard
    Frontend->>Backend: GET /api/auth/session
    Backend-->>Frontend: User Data
    Frontend->>User: Display Authenticated UI
```

## Configuration

The system uses a layered configuration approach with both environment variables and runtime configuration through services.

### Environment Configuration

Basic infrastructure settings:
```bash
# Database Configuration
VECTOR_DB=milvus                 # Vector database type
MILVUS_HOST=localhost           # Vector DB host
MILVUS_PORT=19530              # Vector DB port
DB_HOST=localhost              # PostgreSQL host
DB_PORT=5432                   # PostgreSQL port

# LLM Provider Settings
WATSONX_INSTANCE_ID=your-id    # WatsonX instance ID
WATSONX_APIKEY=your-key        # WatsonX API key
OPENAI_API_KEY=your-key        # OpenAI API key (optional)
ANTHROPIC_API_KEY=your-key     # Anthropic API key (optional)

# Application Settings
EMBEDDING_MODEL=all-minilm-l6-v2  # Default embedding model
DATA_DIR=/path/to/data           # Data directory
```

### Environment Modes

The application supports different operating modes controlled by environment variables:

#### Production Mode (default)
```bash
# All flags false or unset (default)
TESTING=false
SKIP_AUTH=false
DEVELOPMENT_MODE=false
```
- Full authentication required
- OIDC provider registration enabled
- Production security measures enforced

#### Development/CI Mode
```bash
# Any of these set to true activates development mode
TESTING=true              # Set in CI environments
SKIP_AUTH=true           # Skip authentication entirely
DEVELOPMENT_MODE=true    # Local development without auth
```
- Authentication bypassed (test user automatically set)
- OIDC registration skipped (no external connections)
- All endpoints accessible without credentials
- Ideal for testing and local development

#### Testing with Authentication
For testing scenarios that need partial authentication:
```bash
# Use mock token for testing
Authorization: Bearer mock_token_for_testing
```

**Environment Priority**: Any of `TESTING`, `SKIP_AUTH`, or `DEVELOPMENT_MODE` being `true` will activate development mode.

### Service Configuration

Runtime configuration through services:

1. Provider Configuration:
   ```python
   provider_config = ProviderConfigInput(
       provider="watsonx",
       api_key="${WATSONX_APIKEY}",
       project_id="${WATSONX_INSTANCE_ID}",
       active=True
   )
   config_service.create_provider_config(provider_config)
   ```

2. LLM Parameters:
   ```python
   parameters = LLMParametersInput(
       name="default-params",
       provider="watsonx",
       model_id="granite-13b",
       temperature=0.7,
       max_new_tokens=1000
   )
   parameters_service.create_parameters(parameters)
   ```

3. Template Configuration:
   ```python
   template = PromptTemplateInput(
       name="rag-query",
       provider="watsonx",
       template_type=PromptTemplateType.RAG_QUERY,
       template_format="Context:\n{context}\nQuestion:{question}"
   )
   template_service.create_template(template)
   ```

4. Pipeline Configuration:
   ```python
   pipeline_config = PipelineConfigInput(
       name="default-pipeline",
       provider_id=provider.id,
       llm_parameters_id=parameters.id
   )
   pipeline_service.create_pipeline_config(pipeline_config)
   ```

For detailed configuration options and examples, see:
- [Provider Configuration](backend/rag_solution/docs/provider_configuration.md)
- [Configuration Management](backend/rag_solution/docs/configuration.md)
- [Prompt Templates](backend/rag_solution/docs/prompt_templates.md)

## Testing

üéâ **Testing Status**: The project has achieved major testing milestones with 847 tests passing and comprehensive test infrastructure in place.

### Test Framework Status

- **Test Structure**: ‚úÖ Comprehensive test suite implemented
- **Test Execution**: ‚úÖ 847 tests passing, 71 failing (92% success rate)
- **Test Infrastructure**: ‚úÖ pytest fully operational
- **Test Categories**: ‚úÖ Unit, integration, E2E, and atomic tests
- **Code Coverage**: ‚úÖ 50% overall coverage with detailed reporting

### Current Test Results (December 2024)

```
=========================== Test Results ===========================
‚úÖ 847 passed, 71 failed, 42 skipped, 9 warnings
‚è±Ô∏è  Total execution time: 6 minutes
üìä  Code coverage: 50% (6,578/13,065 lines)
```

### Test Categories

| Category | Tests | Status | Coverage |
|----------|-------|--------|----------|
| **Atomic Tests** | 100+ | ‚úÖ Excellent | 9% |
| **Unit Tests** | 83 | ‚úÖ Good | 5% |
| **Integration Tests** | 43 | ‚úÖ Complete | N/A |
| **E2E Tests** | 22 | üîÑ In Progress | N/A |
| **API Tests** | 21 | üîÑ In Progress | 48% |

### Available Test Commands

```bash
# Run all tests
make test

# Run specific test categories
make test-atomic    # Schema and data structure tests
make test-unit      # Business logic tests
make test-integration # Service integration tests
make test-e2e       # End-to-end workflow tests

# Run with coverage
make test-coverage

# Run specific test files
make test testfile=backend/tests/unit/test_conversation_service_tdd.py
```

### Test Infrastructure Features

1. **Comprehensive Coverage**: Tests across all service layers
2. **Fast Execution**: Optimized test suite with parallel execution
3. **Docker Integration**: Tests run in isolated containers
4. **CI/CD Ready**: Automated testing in GitHub Actions
5. **Detailed Reporting**: HTML and XML coverage reports

### Recent Test Optimizations

- **70% reduction** in test files (192 ‚Üí 58 files)
- **Proper test architecture** (atomic ‚Üí unit ‚Üí integration ‚Üí e2e)
- **Fast execution** (total runtime < 10 seconds for core tests)
- **100% code quality compliance** (ruff, mypy, pylint)

For detailed testing information, see:
- [Master Test Optimization Roadmap](docs/tests/MASTER_TEST_OPTIMIZATION_ROADMAP.md)
- [Final Test Optimization Summary](docs/tests/FINAL_TEST_OPTIMIZATION_SUMMARY.md)
- [Testing Documentation](backend/tests/README.md)

## CI/CD

The project uses GitHub Actions for continuous integration and deployment, with automated builds and testing. Images are published to GitHub Container Registry (GHCR).

### Current CI/CD Status

- **Build Pipeline**: ‚úÖ Automated builds for backend and frontend
- **Image Publishing**: ‚úÖ Images published to `ghcr.io/manavgup/rag_modulo/*`
- **Test Execution**: ‚ö†Ô∏è Tests implemented but blocked by authentication issues
- **Quality Checks**: ‚úÖ Code formatting and linting automated

### CI/CD Pipeline

1. **Code Quality**: Automated linting with Ruff and MyPy
2. **Build**: Docker image builds for backend and frontend
3. **Publish**: Images pushed to GHCR with version tags
4. **Testing**: Comprehensive test suite (when authentication is fixed)

### Available Images

- `ghcr.io/manavgup/rag_modulo/backend:latest`
- `ghcr.io/manavgup/rag_modulo/frontend:latest`
- `ghcr.io/manavgup/rag_modulo/backend:test-latest`

### Pipeline Stages

1. Code Quality
   ```yaml
   quality:
     steps:
       - name: Code Formatting
         run: black backend/
       - name: Type Checking
         run: mypy backend/
       - name: Linting
         run: flake8 backend/
       - name: Import Sorting
         run: isort backend/
   ```

2. Testing
   ```yaml
   test:
     steps:
       - name: Unit Tests
         run: pytest backend/tests/services/
       - name: Integration Tests
         run: pytest backend/tests/integration/
       - name: Performance Tests
         run: |
           pytest backend/tests/performance/ \
             --html=performance-report.html
       - name: Coverage Report
         run: |
           pytest --cov=backend/rag_solution \
             --cov-report=xml \
             --cov-fail-under=80
   ```

3. Security
   ```yaml
   security:
     steps:
       - name: Dependency Scan
         run: safety check
       - name: SAST Analysis
         run: bandit -r backend/
       - name: Secret Detection
         run: detect-secrets scan
   ```

4. Build & Deploy
   ```yaml
   deploy:
     steps:
       - name: Build Images
         run: docker-compose build
       - name: Run Tests in Container
         run: docker-compose run test
       - name: Push Images
         run: docker-compose push
   ```

### Quality Gates

The pipeline enforces several quality gates:

1. Code Quality
   - No formatting errors
   - No type checking errors
   - No linting violations
   - Proper import sorting

2. Testing
   - All tests must pass
   - 80% minimum coverage
   - Performance tests within thresholds
   - No integration test failures

3. Security
   - No critical vulnerabilities
   - No exposed secrets
   - Clean SAST scan

4. Service Requirements
   - Service tests pass
   - API contracts validated
   - Configuration validated
   - Performance metrics met

For detailed CI/CD configuration, see:
- [Workflow Configuration](.github/workflows/ci.yml)
- [Docker Compose Configuration](docker-compose.yml)
- [Test Configuration](backend/pytest.ini)

## Contributing

Contributions are welcome! Please follow these guidelines when contributing to the project.

### Development Guidelines

1. Service Layer Architecture
   - Follow the service-based architecture pattern
   - Implement new features as services
   - Use dependency injection
   - Follow repository pattern for data access
   - Document service interfaces

2. Code Style
   - Use type hints
   - Write comprehensive docstrings
   - Follow PEP 8 guidelines
   - Use async/await where appropriate
   - Handle errors properly

3. Testing Requirements
   - Write unit tests for services
   - Add integration tests for flows
   - Include performance tests for critical paths
   - Maintain test coverage above 80%
   - Document test scenarios

### Contribution Process

1. Fork and Clone
   ```bash
   git clone https://github.com/yourusername/rag-modulo.git
   cd rag-modulo
   ```

2. Set Up Development Environment
   ```bash
   # Create virtual environment
   python -m venv venv
   source venv/bin/activate  # or `venv\Scripts\activate` on Windows

   # Install dependencies
   pip install -r requirements.txt
   pip install -r requirements-dev.txt
   ```

3. Create Feature Branch
   ```bash
   git checkout -b feature/YourFeature
   ```

4. Development Workflow
   - Write tests first (TDD)
   - Implement feature
   - Run test suite
   - Update documentation
   - Run linters

5. Testing
   ```bash
   # Run all tests
   pytest

   # Run specific test types
   pytest backend/tests/services/  # Service tests
   pytest backend/tests/integration/  # Integration tests
   pytest backend/tests/performance/  # Performance tests

   # Check coverage
   pytest --cov=backend/rag_solution
   ```

6. Submit Changes
   - Push changes to your fork
   - Create pull request
   - Fill out PR template
   - Respond to reviews

### Documentation

When adding new features:
1. Update service documentation
2. Add configuration examples
3. Update testing documentation
4. Include performance considerations
5. Document API changes

For detailed development guidelines, see:
- [Development Guide](backend/rag_solution/docs/development_prompt.md)
- [Testing Guide](backend/tests/README.md)
- [Configuration Guide](backend/rag_solution/docs/configuration.md)

## üó∫Ô∏è Project Roadmap

### ‚úÖ Phase 1: Foundation & Testing (COMPLETED)

**Achievements**: Built solid foundation with comprehensive testing infrastructure

1. **‚úÖ Test Infrastructure** - 847 tests passing, comprehensive coverage
2. **‚úÖ Core Services** - Search, conversation, and token tracking operational
3. **‚úÖ Development Workflow** - Streamlined Docker-based development
4. **‚úÖ CI/CD Pipeline** - Automated builds and testing
5. **‚úÖ Documentation** - Comprehensive documentation across all components

### üîÑ Phase 2: Test Optimization & Bug Fixes (CURRENT)

**Priority**: Fix remaining test failures and optimize performance

1. **Fix Remaining Tests** (IN PROGRESS)
   - Resolve 71 failing tests (down from 200+)
   - Fix API endpoint integration issues
   - Resolve CLI testing environment problems
   - Optimize E2E test reliability

2. **Performance Optimization** (IN PROGRESS)
   - Improve test execution speed
   - Optimize database queries
   - Enhance memory usage
   - Streamline API responses

3. **Code Quality Enhancement** (IN PROGRESS)
   - Increase test coverage to 80%
   - Improve error handling
   - Enhance logging and monitoring
   - Refactor complex components

### üöÄ Phase 3: Production Readiness (NEXT)

**Target**: Production-ready system with full functionality

1. **Production Deployment** - Set up production infrastructure
2. **Monitoring & Observability** - Comprehensive monitoring and alerting
3. **Security Hardening** - Security audit and hardening
4. **Performance Tuning** - Optimize for production workloads

### üîÆ Phase 4: Advanced Features (FUTURE)

1. **Agentic AI Enhancement** - Transform into autonomous AI system
2. **Advanced Reasoning** - Enhanced chain of thought capabilities
3. **Multi-Modal Support** - Image and document processing
4. **Enterprise Features** - Advanced security and compliance

## üö® Known Issues

### High Priority Issues
- **API Integration**: 21 chat router tests failing (validation and routing issues)
- **Token Tracking E2E**: 5 E2E tests failing (response format issues)
- **CLI Testing**: 8 CLI tests failing (environment setup issues)
- **Unit Test Failures**: 37 unit tests failing (service integration issues)

### Medium Priority Issues
- **Test Coverage**: Overall coverage at 50% (target: 80%)
- **Performance**: Some tests taking longer than expected
- **Error Handling**: Some edge cases not properly handled
- **Documentation**: Some API documentation needs updates

### Low Priority Issues
- **Code Quality**: Minor linting issues in some files
- **Logging**: Some services need better logging
- **Monitoring**: Enhanced monitoring and alerting needed

## ü§ù Contributing

**üéâ Great News**: The project is now in a much better state with 847 tests passing and core functionality working! We welcome contributions to help fix the remaining 71 failing tests and improve the system further.

### Development Guidelines

1. **Service Layer Architecture**
   - Follow the service-based architecture pattern
   - Implement new features as services
   - Use dependency injection
   - Follow repository pattern for data access

2. **Code Style**
   - Use type hints throughout
   - Write comprehensive docstrings
   - Follow PEP 8 guidelines
   - Use async/await where appropriate

3. **Testing Requirements**
   - Write unit tests for services
   - Add integration tests for flows
   - Include performance tests for critical paths
   - Maintain test coverage above 80%

### Contribution Process

1. **Fork and Clone**
2. **Set Up Development Environment** (when authentication is fixed)
3. **Create Feature Branch**
4. **Development Workflow** - Write tests first (TDD)
5. **Testing** - Run test suite
6. **Submit Changes** - Create pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Troubleshooting

### Critical Issues (Current Blockers)

#### Authentication System Not Working

**Problem**: OIDC authentication is broken, blocking all API testing and functionality verification.

**Symptoms**:
- Login attempts fail
- API endpoints return authentication errors
- Cannot test any RAG functionality

**Status**: This is the #1 priority issue that needs to be resolved before any other development can proceed.

#### Local Development Environment Issues

**Problem**: Dependency issues preventing local development setup.

**Symptoms**:
- Poetry installation fails
- pytest not available
- Import errors in local environment

**Temporary Workaround**: Use Docker containers for development:
```bash
make run-ghcr  # Use pre-built images
```

#### Testing Framework Not Available

**Problem**: pytest and testing tools not properly installed.

**Symptoms**:
- `make test` commands fail
- Cannot run any tests
- Test coverage reports unavailable

**Status**: Depends on fixing local development environment.

### Container Issues

#### Service Health Check Failures

If services fail to become healthy:
```bash
# Check service logs
make logs

# Restart services
make stop-containers
make run-services

# Check individual container health
docker compose ps
```

#### GHCR Image Pull Issues

If you have issues pulling images from GitHub Container Registry:
```bash
# Login to GHCR (if needed)
docker login ghcr.io

# Pull latest images
make pull-ghcr-images
```

### Performance Issues

For large datasets or high concurrency:
- Increase memory limits in docker-compose files
- Adjust vector database configuration
- Monitor resource usage with `docker stats`

### Getting Help

1. **Check the logs**: `make logs` to see container logs
2. **Verify environment**: Ensure all required environment variables are set
3. **Check container health**: `docker compose ps` to see service status
4. **Review documentation**: Check the detailed documentation in `claudeDev_Docs/`

### Known Workarounds

- **For Development**: Use `make run-ghcr` instead of local builds
- **For Testing**: Wait for authentication system to be fixed
- **For Local Setup**: Use Docker containers until local environment is fixed
