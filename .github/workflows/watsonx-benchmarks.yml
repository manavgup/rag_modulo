name: WatsonX Performance Benchmarks

on:
  # Run benchmarks on schedule
  schedule:
    - cron: '0 0 * * 0'  # Run weekly at midnight on Sunday
  
  # Allow manual trigger
  workflow_dispatch:
  
  # Run on PRs that modify WatsonX provider or benchmarks
  pull_request:
    paths:
      - 'backend/rag_solution/generation/providers/watsonx.py'
      - 'backend/tests/performance/test_watsonx_benchmarks.py'
      - '.github/workflows/watsonx-benchmarks.yml'

jobs:
  check_credentials:
    runs-on: ubuntu-latest
    outputs:
      has_credentials: ${{ steps.check.outputs.has_credentials }}
    steps:
      - id: check
        run: |
          if [ "${{ secrets.WX_API_KEY }}" != "" ]; then
            echo "has_credentials=true" >> $GITHUB_OUTPUT
          else
            echo "has_credentials=false" >> $GITHUB_OUTPUT
          fi

  benchmark:
    needs: check_credentials
    if: needs.check_credentials.outputs.has_credentials == 'true'
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    env:
      PYTHON_VERSION: '3.10'
      POETRY_VERSION: '1.4.2'
      WX_API_KEY: ${{ secrets.WX_API_KEY }}
      WX_URL: ${{ secrets.WX_URL }}
      WX_PROJECT_ID: ${{ secrets.WX_PROJECT_ID }}
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for trend analysis
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 - --version ${{ env.POETRY_VERSION }}
          echo "$HOME/.local/bin" >> $GITHUB_PATH
      
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true
      
      - name: Cache Poetry virtualenv
        uses: actions/cache@v4
        with:
          path: ./.venv
          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
      
      - name: Install dependencies
        run: |
          cd backend
          poetry install
      
      - name: Load previous benchmark results
        uses: actions/cache@v4
        with:
          path: backend/benchmark_results.json
          key: benchmark-results-${{ github.ref }}
          restore-keys: |
            benchmark-results-
      
      - name: Run benchmarks
        run: |
          cd backend
          poetry run pytest tests/performance/test_watsonx_benchmarks.py -v --benchmark-only
      
      - name: Analyze benchmark results
        id: analysis
        run: |
          cd backend
          # Run analysis and capture output
          OUTPUT=$(poetry run pytest tests/performance/test_watsonx_benchmarks.py::test_analyze_performance_trends -v)
          echo "ANALYSIS<<EOF" >> $GITHUB_ENV
          echo "$OUTPUT" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
          
          # Check for significant performance changes
          if echo "$OUTPUT" | grep -q "Performance change detected"; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Save benchmark results
        uses: actions/cache/save@v4
        with:
          path: backend/benchmark_results.json
          key: benchmark-results-${{ github.ref }}-${{ github.sha }}
      
      - name: Generate benchmark report
        run: |
          cd backend
          echo "# WatsonX Performance Benchmark Report" > benchmark_report.md
          echo "## Run Details" >> benchmark_report.md
          echo "- Date: $(date)" >> benchmark_report.md
          echo "- Commit: ${{ github.sha }}" >> benchmark_report.md
          echo "- Branch: ${{ github.ref }}" >> benchmark_report.md
          echo "" >> benchmark_report.md
          echo "## Performance Analysis" >> benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          echo "${{ env.ANALYSIS }}" >> benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
      
      # Create/update PR comment with benchmark results
      - name: Comment PR
        if: github.event_name == 'pull_request' && steps.analysis.outputs.has_changes == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('backend/benchmark_report.md', 'utf8');
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const benchmarkComment = comments.find(comment => 
              comment.body.startsWith('# WatsonX Performance Benchmark Report')
            );
            
            if (benchmarkComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: benchmarkComment.id,
                body: report
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }
      
      # Create issue for significant performance changes in scheduled runs
      - name: Create Performance Alert Issue
        if: |
          github.event_name == 'schedule' && 
          steps.analysis.outputs.has_changes == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('backend/benchmark_report.md', 'utf8');
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '⚠️ WatsonX Performance Changes Detected',
              body: report,
              labels: ['performance', 'automated']
            });
      
      # Upload benchmark results as artifact
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            backend/benchmark_results.json
            backend/benchmark_report.md
