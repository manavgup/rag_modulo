name: Claude Code Review

on:
  pull_request:
    types: [opened, synchronize]
    # Optional: Only run on specific file changes
    # paths:
    #   - "src/**/*.ts"
    #   - "src/**/*.tsx"
    #   - "src/**/*.js"
    #   - "src/**/*.jsx"

jobs:
  claude-review:
    # Optional: Filter by PR author
    # if: |
    #   github.event.pull_request.user.login == 'external-contributor' ||
    #   github.event.pull_request.user.login == 'new-developer' ||
    #   github.event.pull_request.author_association == 'FIRST_TIME_CONTRIBUTOR'

    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Check if AI-Generated PR
        id: check-ai-generated
        run: |
          PR_LABELS=$(gh pr view ${{ github.event.pull_request.number }} --json labels --jq '.labels[].name' | tr '\n' ',' || echo "")
          if echo "$PR_LABELS" | grep -q "ai-generated"; then
            echo "is_ai_generated=true" >> $GITHUB_OUTPUT
          else
            echo "is_ai_generated=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run Claude Code Review
        id: claude-review
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          prompt: |
            ${{ steps.check-ai-generated.outputs.is_ai_generated == 'true' && format('
            ‚ö†Ô∏è **IMPORTANT: This PR was generated by AI (Google Gemini)**

            Apply EXTRA scrutiny and use the enhanced review checklist below.

            ## ü§ñ AI-Specific Review Checklist

            **Critical Checks:**
            1. **Task Alignment**: Does the code actually solve the linked issue? (AI sometimes drifts off-task)
            2. **Hallucination Detection**: Are all functions, imports, and APIs real? (AI may invent non-existent code)
            3. **Architecture Understanding**: Does the code fit the existing architecture? (Check patterns, naming, structure)
            4. **Over-Engineering**: Is the solution unnecessarily complex? (AI tends to over-complicate)

            **Security & Safety:**
            5. **Input Validation**: Are all user inputs properly validated?
            6. **SQL Injection**: Any direct SQL queries without parameterization?
            7. **XSS Vulnerabilities**: Any unsanitized user input in templates?
            8. **Authentication/Authorization**: Are security checks present where needed?
            9. **Error Handling**: Are errors handled comprehensively? (AI often skips edge cases)
            10. **Secrets Management**: No hardcoded credentials or API keys?

            **Code Quality:**
            11. **Error Messages**: Are error messages specific and actionable? (AI writes vague errors)
            12. **Edge Cases**: Are boundary conditions handled? (null, empty, negative, huge values)
            13. **Type Safety**: Proper type hints and validation? (Python)
            14. **Naming Consistency**: Do names match existing conventions?
            15. **Code Duplication**: Any copy-paste errors or inconsistent patterns?

            **Testing:**
            16. **Test Coverage**: Are tests comprehensive or superficial?
            17. **Test Quality**: Do tests actually verify behavior or just call functions?
            18. **Edge Case Tests**: Are error cases and boundaries tested?
            19. **Test Independence**: Can tests run in isolation?

            **Performance:**
            20. **Efficiency**: Any obvious performance issues? (N+1 queries, inefficient loops)
            21. **Resource Management**: Are connections/files properly closed?

            ## üö® When to Block AI PRs

            **Reject if you find:**
            - Hallucinated functions or non-existent imports
            - Security vulnerabilities (SQL injection, XSS, etc.)
            - Code that doesn'\''t solve the actual issue
            - Missing critical error handling
            - Tests that don'\''t actually test anything

            **Recommend manual review if:**
            - Complex business logic (AI struggles with multi-step reasoning)
            - Security-sensitive code (auth, payment, data validation)
            - Performance-critical sections
            - More than 10 files changed

            ## üìù Review Output Format

            Use this structure in your review comment:

            ```markdown
            ## ü§ñ AI Code Review

            **Overall Assessment**: [APPROVE / REQUEST CHANGES / NEEDS MANUAL REVIEW]

            ### ‚úÖ Strengths
            - [What the AI did well]

            ### ‚ö†Ô∏è Issues Found
            1. **[Severity]** [Issue description]
               - Location: `file.py:123`
               - Fix: [Specific fix needed]

            ### üîç Recommendations
            - [Suggestions for improvement]

            ### üß™ Testing Notes
            - [Comments on test quality]

            ---
            *This PR was reviewed with enhanced AI-generated code scrutiny.*
            ```

            ') || 'Please review this pull request and provide feedback on:
            - Code quality and best practices
            - Potential bugs or issues
            - Performance considerations
            - Security concerns
            - Test coverage' }}

            Use the repository's CLAUDE.md for guidance on style and conventions. Be constructive and helpful in your feedback.

            Use `gh pr comment` with your Bash tool to leave your review as a comment on the PR.

          # See https://github.com/anthropics/claude-code-action/blob/main/docs/usage.md
          # or https://docs.anthropic.com/en/docs/claude-code/sdk#command-line for available options
          claude_args: '--allowed-tools "Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*)"'
